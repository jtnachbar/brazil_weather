{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "# import data\n",
    "data = pickle.load(open(\"data.p\", \"rb\"))\n",
    "labels = pickle.load(open(\"labels.p\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize the data by feature\n",
    "\n",
    "for i in range(len(data[0])):\n",
    "    # Calculate the mean and std for each attribute\n",
    "    mean = np.mean(data[0:, i])\n",
    "    std = np.std(data[0:, i])\n",
    "    # subtract the mean and divide by the std for each attribute\n",
    "    for j in range(len(data[0:, i])):\n",
    "        data[j][i] -= mean\n",
    "        data[j][i] /= std\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into train and test\n",
    "train_data, test_data, train_labels, test_labels = train_test_split(data, labels, test_size=0.1, random_state=10)\n",
    "train_data = torch.from_numpy(train_data)\n",
    "test_data = torch.from_numpy(test_data)\n",
    "train_labels = torch.from_numpy(train_labels)\n",
    "test_labels = torch.from_numpy(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the network architecture\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(64, 128)\n",
    "        self.dropout1 = nn.Dropout(0.8)\n",
    "        self.fc2 = nn.Linear(128, 16)\n",
    "        self.dropout2 = nn.Dropout(0.8)\n",
    "        self.fc3 = nn.Linear(16, 1)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        output = self.fc3(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 5\n",
    "batch_size = 1\n",
    "g = 0.7\n",
    "learn = 1.0\n",
    "\n",
    "def train(model, data, labels, device, optimizer, epoch, log_int):\n",
    "    model.train()\n",
    "    for i in range(0, int(len(data)/batch_size)):\n",
    "        inp = data[i*batch_size:(i+1)*batch_size].to(device).float()\n",
    "        target = labels[i*batch_size:(i+1)*batch_size].to(device).float()\n",
    "        optimizer.zero_grad()\n",
    "        out = model(inp)\n",
    "        loss = nn.MSELoss()(out, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if i % log_int == 0:\n",
    "            print('Train Epoch: ' + str(epoch) + ' | Loss: ' + str(loss.item()))\n",
    "    # For each batch in the data\n",
    "        # Reset the gradient\n",
    "        # Run the batch through the model\n",
    "        # Comnpute the loss\n",
    "        # Propogate the loss\n",
    "        # Move down the gradient\n",
    "        # Idk maybe print training progress?\n",
    "def test(model, data, labels, device):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, int(len(data)/batch_size)):\n",
    "            inp = data[i*batch_size:(i+1)*batch_size].to(device).float()\n",
    "            target = labels[i*batch_size:(i+1)*batch_size].to(device).float()\n",
    "            output = model(inp)\n",
    "            test_loss += nn.L1Loss()(output, target)\n",
    "    print(\"Test loss: \" + str(test_loss / len(data)))\n",
    "        # For each entry in the test data set\n",
    "            # Run the entry through and add the loss to our total loss\n",
    "        # Divide by the total length to get the average error\n",
    "\n",
    "if torch.cude.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "model = Net().to(device)\n",
    "optimizer = optim.Adadelta(model.parameters(), lr=learn)\n",
    "scheduler = StepLR(optimizer, step_size=1, gamma=g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jnachbar/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:431: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 | Loss: 0.016386058181524277\n",
      "Train Epoch: 1 | Loss: 0.00027932444936595857\n",
      "Train Epoch: 1 | Loss: 2.9458062272169627e-05\n",
      "Train Epoch: 1 | Loss: 0.0002511856728233397\n",
      "Train Epoch: 1 | Loss: 0.01555953174829483\n",
      "Train Epoch: 1 | Loss: 57.47211456298828\n",
      "Train Epoch: 1 | Loss: 1.1224027872085571\n",
      "Train Epoch: 1 | Loss: 0.17688564956188202\n",
      "Train Epoch: 1 | Loss: 0.02023434266448021\n",
      "Train Epoch: 1 | Loss: 0.037809960544109344\n",
      "Train Epoch: 1 | Loss: 0.018651815131306648\n",
      "Train Epoch: 1 | Loss: 0.022756243124604225\n",
      "Train Epoch: 1 | Loss: 0.00014065054710954428\n",
      "Train Epoch: 1 | Loss: 0.045079249888658524\n",
      "Train Epoch: 1 | Loss: 0.001953916857019067\n",
      "Train Epoch: 1 | Loss: 2.0110056400299072\n",
      "Train Epoch: 1 | Loss: 0.01937069557607174\n",
      "Train Epoch: 1 | Loss: 0.006498256698250771\n",
      "Train Epoch: 1 | Loss: 1.2359732389450073\n",
      "Train Epoch: 1 | Loss: 0.02734832651913166\n",
      "Train Epoch: 1 | Loss: 0.0003293673798907548\n",
      "Train Epoch: 1 | Loss: 0.01603417843580246\n",
      "Train Epoch: 1 | Loss: 7.31638765335083\n",
      "Train Epoch: 1 | Loss: 0.01619940809905529\n",
      "Train Epoch: 1 | Loss: 0.0017815103055909276\n",
      "Train Epoch: 1 | Loss: 0.0014619027497246861\n",
      "Train Epoch: 1 | Loss: 0.0017692068358883262\n",
      "Train Epoch: 1 | Loss: 0.018421819433569908\n",
      "Train Epoch: 1 | Loss: 0.01586664468050003\n",
      "Train Epoch: 1 | Loss: 0.013398981653153896\n",
      "Train Epoch: 1 | Loss: 0.01183883287012577\n",
      "Train Epoch: 1 | Loss: 0.029708916321396828\n",
      "Train Epoch: 1 | Loss: 0.013652239926159382\n",
      "Train Epoch: 1 | Loss: 0.000268043193500489\n",
      "Train Epoch: 1 | Loss: 0.0002950396155938506\n",
      "Train Epoch: 1 | Loss: 0.02057875506579876\n",
      "Train Epoch: 1 | Loss: 0.010447371751070023\n",
      "Train Epoch: 1 | Loss: 0.015246283262968063\n",
      "Train Epoch: 1 | Loss: 0.023145176470279694\n",
      "Train Epoch: 1 | Loss: 0.00901690125465393\n",
      "Train Epoch: 1 | Loss: 0.0014086371520534158\n",
      "Train Epoch: 1 | Loss: 0.013764828443527222\n",
      "Train Epoch: 1 | Loss: 0.018110284581780434\n",
      "Train Epoch: 1 | Loss: 0.001949637895449996\n",
      "Train Epoch: 1 | Loss: 0.02521473541855812\n",
      "Train Epoch: 1 | Loss: 0.023697352036833763\n",
      "Train Epoch: 1 | Loss: 0.008044980466365814\n",
      "Train Epoch: 1 | Loss: 0.031270768493413925\n",
      "Train Epoch: 1 | Loss: 0.04378104209899902\n",
      "Train Epoch: 1 | Loss: 0.07404495030641556\n",
      "Train Epoch: 1 | Loss: 0.020028360188007355\n",
      "Train Epoch: 1 | Loss: 0.06152547150850296\n",
      "Train Epoch: 1 | Loss: 0.005028747022151947\n",
      "Train Epoch: 1 | Loss: 0.013402378186583519\n",
      "Train Epoch: 1 | Loss: 0.004434485454112291\n",
      "Train Epoch: 1 | Loss: 0.01752893254160881\n",
      "Train Epoch: 1 | Loss: 0.01003981288522482\n",
      "Train Epoch: 1 | Loss: 0.0014473276678472757\n",
      "Train Epoch: 1 | Loss: 0.007134836632758379\n",
      "Train Epoch: 1 | Loss: 0.023462871089577675\n",
      "Train Epoch: 1 | Loss: 12.963109970092773\n",
      "Train Epoch: 1 | Loss: 0.006547941826283932\n",
      "Train Epoch: 1 | Loss: 0.015327109955251217\n",
      "Train Epoch: 1 | Loss: 0.054990965873003006\n",
      "Train Epoch: 1 | Loss: 0.033447008579969406\n",
      "Train Epoch: 1 | Loss: 0.0015966370701789856\n",
      "Train Epoch: 1 | Loss: 0.0009108409285545349\n",
      "Train Epoch: 1 | Loss: 0.022974533960223198\n",
      "Train Epoch: 1 | Loss: 0.036917153745889664\n",
      "Train Epoch: 1 | Loss: 0.0005830079899169505\n",
      "Train Epoch: 1 | Loss: 0.03568067401647568\n",
      "Train Epoch: 1 | Loss: 0.002587155904620886\n",
      "Train Epoch: 1 | Loss: 0.014590475708246231\n",
      "Train Epoch: 1 | Loss: 0.0011660844320431352\n",
      "Train Epoch: 1 | Loss: 0.07510767877101898\n",
      "Train Epoch: 1 | Loss: 0.00017950937035493553\n",
      "Train Epoch: 1 | Loss: 1.700575590133667\n",
      "Train Epoch: 1 | Loss: 0.0045942096039652824\n",
      "Train Epoch: 1 | Loss: 0.010810384526848793\n",
      "Train Epoch: 1 | Loss: 0.046728625893592834\n",
      "Train Epoch: 1 | Loss: 0.00026773978606797755\n",
      "Train Epoch: 1 | Loss: 9.162500646198168e-06\n",
      "Train Epoch: 1 | Loss: 0.04636330157518387\n",
      "Train Epoch: 1 | Loss: 0.004486612509936094\n",
      "Train Epoch: 1 | Loss: 0.0058000050485134125\n",
      "Train Epoch: 1 | Loss: 0.014243354089558125\n",
      "Train Epoch: 1 | Loss: 0.008642698638141155\n",
      "Train Epoch: 1 | Loss: 0.009465853683650494\n",
      "Train Epoch: 1 | Loss: 0.013875671662390232\n",
      "Train Epoch: 1 | Loss: 0.02721559628844261\n",
      "Train Epoch: 1 | Loss: 0.030636845156550407\n",
      "Train Epoch: 1 | Loss: 0.022858351469039917\n",
      "Train Epoch: 1 | Loss: 0.04121905192732811\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jnachbar/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:88: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: tensor(0.3348)\n",
      "Train Epoch: 1 | Loss: 0.005363671574741602\n",
      "Train Epoch: 1 | Loss: 0.011684668250381947\n",
      "Train Epoch: 1 | Loss: 0.010864540934562683\n",
      "Train Epoch: 1 | Loss: 0.041237153112888336\n",
      "Train Epoch: 1 | Loss: 0.0012942473404109478\n",
      "Train Epoch: 1 | Loss: 55.342533111572266\n",
      "Train Epoch: 1 | Loss: 1.1085481643676758\n",
      "Train Epoch: 1 | Loss: 0.0015874427044764161\n",
      "Train Epoch: 1 | Loss: 0.04119438678026199\n",
      "Train Epoch: 1 | Loss: 0.0009842320578172803\n",
      "Train Epoch: 1 | Loss: 0.041619110852479935\n",
      "Train Epoch: 1 | Loss: 0.00440789433196187\n",
      "Train Epoch: 1 | Loss: 0.045777563005685806\n",
      "Train Epoch: 1 | Loss: 0.0022350798826664686\n",
      "Train Epoch: 1 | Loss: 0.0039232391864061356\n",
      "Train Epoch: 1 | Loss: 1.489198088645935\n",
      "Train Epoch: 1 | Loss: 0.0046398211270570755\n",
      "Train Epoch: 1 | Loss: 0.020318618044257164\n",
      "Train Epoch: 1 | Loss: 0.9216700196266174\n",
      "Train Epoch: 1 | Loss: 0.03534887358546257\n",
      "Train Epoch: 1 | Loss: 0.02443225495517254\n",
      "Train Epoch: 1 | Loss: 0.06107354164123535\n",
      "Train Epoch: 1 | Loss: 6.782352447509766\n",
      "Train Epoch: 1 | Loss: 0.00438946858048439\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-143-aa2f9068b56e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-142-c5e7c55ccf86>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, data, labels, device, optimizer, epoch, log_int)\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMSELoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    429\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    430\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 431\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    432\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    433\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mmse_loss\u001b[0;34m(input, target, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m   2201\u001b[0m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mreduction\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'mean'\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2202\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2203\u001b[0;31m         \u001b[0mexpanded_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpanded_target\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbroadcast_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2204\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpanded_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpanded_target\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2205\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/functional.py\u001b[0m in \u001b[0;36mbroadcast_tensors\u001b[0;34m(*tensors)\u001b[0m\n\u001b[1;32m     50\u001b[0m                 [0, 1, 2]])\n\u001b[1;32m     51\u001b[0m     \"\"\"\n\u001b[0;32m---> 52\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_VariableFunctions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbroadcast_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    train(model, train_data, train_labels, device, optimizer, 1, 1000)\n",
    "    test(model, test_data, test_labels, device)\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: tensor(0.0123)\n"
     ]
    }
   ],
   "source": [
    "test(model, test_data, test_labels, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.05606008329882872"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "reg = LinearRegression().fit(train_data, train_labels)\n",
    "reg.score(train_data, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = reg.predict(test_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
