{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(\"Data/sudeste.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = data[data['city'] == 'Rio de Janeiro']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "zeroes = s[s['prcp'] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36498"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(zeroes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add stuff about looking at all the different weather stations in Rio (Maybe even make a cute map)\n",
    "m = s[s['wsnm'] == 'MARAMBAIA']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['wsid', 'wsnm', 'elvt', 'lat', 'lon', 'inme', 'city', 'prov', 'mdct',\n",
       "       'date', 'yr', 'mo', 'da', 'hr', 'prcp', 'stp', 'smax', 'smin', 'gbrd',\n",
       "       'temp', 'dewp', 'tmax', 'dmax', 'tmin', 'dmin', 'hmdy', 'hmax', 'hmin',\n",
       "       'wdsp', 'wdct', 'gust'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This dataframe contains observations from Marambaia, selected for relevant information\n",
    "m_cols = m[['prcp', 'stp', 'temp', 'dewp', 'hmdy', 'wdsp', 'wdct', 'gust']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merging different weather stations together\n",
    "\n",
    "It probably makes sense to just use Marambaia, since it covers the entire time period. We could potentially incorporate data fron the other weather stations though, if we want a way to deal with any gaps in the data it could be a good way to supplement. The next step is to take the data, turn it into a numpy array, and then load it into two lists, one containing all the relevant data points, and the other containing the labels (rainfall in the next 4 hours)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "npArr = m_cols.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "labels = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The weather station periodically goes offline, so we want to exclude some data points\n",
    "def isValidRows(arr, i, numRows):\n",
    "    for j in range(numRows):\n",
    "        if arr[i+j][0] == 0:\n",
    "            return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "100000\n",
      "110000\n",
      "120000\n"
     ]
    }
   ],
   "source": [
    "# Go through every row, finding a value that has 10 valid rows in order. If this is the case, \n",
    "# add the next 8 rows to data, and the sum of the rain for the last 2 rows to labels\n",
    "for i in range(len(npArr) - 10):\n",
    "    if(i % 10000 == 0):\n",
    "        print(i)\n",
    "    if isValidRows(npArr, i, 10):\n",
    "        data.append(npArr[i:i+8].flatten())\n",
    "        labels.append(np.nan_to_num(npArr[i+9][0]) + np.nan_to_num(npArr[i+10][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "102424"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = np.array(labels)\n",
    "len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace the nan's with 0's\n",
    "data = np.nan_to_num(data)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the data as a pickle\n",
    "import pickle\n",
    "\n",
    "pickle.dump(data, open(\"data.p\", \"wb\"))\n",
    "pickle.dump(labels, open(\"labels.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our model\n",
    "\n",
    "# Train the model, testing once per epoch"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
